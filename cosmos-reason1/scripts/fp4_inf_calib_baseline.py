#!/usr/bin/env python3
"""
Cosmos-Reason1-7B FP4 Inference Pipeline (Improved Accuracy)
------------------------------------------------------------
Improvements:
1. Uses REAL calibration data (videos + text) instead of random noise.
2. Protects sensitive layers (lm_head, first/last layers) from destructive quantization.
3. Uses a proper chat template for the calibration prompts.
"""

import os
import sys
import logging
import warnings
import argparse
import time
import gc
import glob
from pathlib import Path
from typing import List, Dict, Any, Generator

import numpy as np
import torch
from transformers import AutoProcessor, AutoTokenizer

# --- LOGGING SETUP ---
os.environ["VLLM_LOGGING_LEVEL"] = "ERROR"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
logging.basicConfig(level=logging.INFO) # Changed to INFO to see calibration progress
logger = logging.getLogger("fp4_calibration")

warnings.filterwarnings("ignore")

try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("‚ùå Error: 'vllm' not found. Install it via: pip install vllm")
    sys.exit(1)

try:
    from llmcompressor.modifiers.quantization import QuantizationModifier
    from llmcompressor.pipelines import oneshot
except ImportError:
    print("‚ùå Error: 'llmcompressor' not found. Install it via: pip install llmcompressor")
    sys.exit(1)

# ============================================================
# Configuration
# ============================================================
MODEL_ID = "nvidia/Cosmos-Reason1-7B"
QUANT_PATH = "Cosmos-Reason1-7B-NVFP4-Accurate" # Changed output folder name

# Calibration settings
CALIB_NUM_SAMPLES = 16  # Number of real video samples to use
CALIB_MAX_SEQ_LEN = 2048 

# ============================================================
# 1. Calibration Data Loader (THE FIX)
# ============================================================
def get_calibration_loader(processor, video_dir: str, num_samples: int = 8):
    """
    Generates REAL input data for calibration.
    Reads actual videos from the directory and pairs them with generic prompts.
    """
    video_files = glob.glob(os.path.join(video_dir, "*.mp4")) + \
                  glob.glob(os.path.join(video_dir, "*.mkv"))
    
    if not video_files:
        raise ValueError(f"No video files found in {video_dir} for calibration!")

    print(f"üìπ Found {len(video_files)} videos. Using {min(len(video_files), num_samples)} for calibration.")

    # Generic prompts to trigger typical activation patterns
    prompts = [
        "Describe the events in this video in detail.",
        "What is happening in this scene?",
        "Explain the physics observed in this video.",
        "Is there any anomaly in this footage?",
        "List the objects visible in the video."
    ]

    count = 0
    import decord # Lazy import
    from decord import VideoReader, cpu
    
    # We cycle through videos and prompts to create diverse calibration data
    while count < num_samples:
        for vid_path in video_files:
            if count >= num_samples:
                break
                
            prompt_text = prompts[count % len(prompts)]
            
            # Prepare messages (Chat Format)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "video": vid_path},
                        {"type": "text", "text": prompt_text},
                    ],
                }
            ]
            
            # Prepare inputs using the standard processor
            # This handles video loading, resizing, and tokenization correctly
            try:
                text_input = processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                # Load video frames using decord manually to ensure control or rely on processor
                # Here we rely on qwen_vl_utils internally or similar logic if using qwen processor
                # But standard HF AutoProcessor for Cosmos usually expects 'videos' argument as list of paths or tensors
                
                # NOTE: For Cosmos/Qwen2-VL, we often need qwen_vl_utils process_vision_info
                # But for calibration with llmcompressor, we need raw torch inputs.
                # Let's use the processor's built-in handling for simplicity.
                
                inputs = processor(
                    text=[text_input],
                    videos=[vid_path],
                    padding=True,
                    return_tensors="pt",
                )
                
                # Move to CPU first (llmcompressor moves to GPU internally)
                # We yield a dictionary of keyword arguments for model.forward()
                batch = {
                    "input_ids": inputs.input_ids,
                    "attention_mask": inputs.attention_mask,
                    "pixel_values": inputs.pixel_values,
                    "image_grid_thw": inputs.image_grid_thw
                }
                
                # Qwen2-VL specific args usually also include 'pixel_values_videos' sometimes
                # Check your specific processor output keys. 
                # For Cosmos (Qwen2.5-VL based), the above are standard.
                
                yield batch
                count += 1
                print(f"   Generated calibration sample {count}/{num_samples}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Skipping {vid_path} due to error: {e}")
                continue

# ============================================================
# 2. Quantization Routine
# ============================================================
def ensure_fp4_model(video_dir_for_calib: str):
    if os.path.exists(QUANT_PATH):
        print(f"‚úÖ Found existing quantized model at: {QUANT_PATH}")
        return

    print(f"‚ö° Quantizing {MODEL_ID} to FP4 with REAL calibration data...")
    
    # Load Processor & Model
    # TRUST_REMOTE_CODE is often needed for Qwen/Cosmos models
    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    # Define targets to IGNORE
    # 1. Visual Encoder (already done, but let's be explicit)
    # 2. LM Head (Keep high precision for output stability)
    ignore_targets = ["re:.*visual.*", "lm_head"] 

    # Define the Quantization Recipe
    recipe = QuantizationModifier(
        targets="Linear",
        scheme="FP4", # NVFP4 target
        ignore=ignore_targets, 
        dampening_frac=0.01, # Helps stability slightly
    )

    # Create the data loader
    # NOTE: We pass the generator function, NOT the generator object if using some libraries, 
    # but llmcompressor usually takes an iterable.
    def data_loader_func():
        return get_calibration_loader(processor, video_dir_for_calib, CALIB_NUM_SAMPLES)

    # Run One-Shot
    print("   Starting One-Shot calibration...")
    oneshot(
        model=MODEL_ID,
        dataset=data_loader_func(), # Pass the iterable of batches
        recipe=recipe,
        output_dir=QUANT_PATH,
        trust_remote_code=True,
        # Reduce memory usage during calibration
        max_seq_length=CALIB_MAX_SEQ_LEN,
    )
    
    print(f"üéâ Quantization complete! Saved to {QUANT_PATH}")

    # Force cleanup
    gc.collect()
    torch.cuda.empty_cache()

# ============================================================
# 3. Inference Routine (Unchanged mostly)
# ============================================================
def run_inference(video_dir: str):
    print(f"üöÄ Loading vLLM from: {QUANT_PATH}")
    
    # Load vLLM with specific quantization config
    llm = LLM(
        model=QUANT_PATH,
        trust_remote_code=True,
        quantization="fp4",  # Ensure vLLM knows it's FP4
        max_model_len=4096,
        gpu_memory_utilization=0.9,
        limit_mm_per_prompt={"image": 5, "video": 2}, # Adjust based on 5090 VRAM
    )

    video_files = glob.glob(os.path.join(video_dir, "*.mp4"))
    sampling_params = SamplingParams(
        temperature=0.1, # Low temp for factual reasoning
        top_p=0.9,
        max_tokens=128
    )

    PROMPT = "Explain the physics and events in this video."

    print(f"üìÇ Processing {len(video_files)} videos...")
    
    for video_file in video_files:
        video_path = str(Path(video_file).absolute())
        
        # Prepare inputs for vLLM
        # vLLM handles the processing internally given the prompt structure
        inputs = {
            "prompt": f"<|user|>\n<|video|>\n{PROMPT}\n<|end|>\n<|assistant|>\n",
            "multi_modal_data": {
                "video": video_path
            }
        }

        try:
            outputs = llm.generate([inputs], sampling_params=sampling_params, use_tqdm=False)
            generated_text = outputs[0].outputs[0].text.strip()
            print(f"[{Path(video_file).name}] -> {generated_text}")
        except Exception as e:
            print(f"Error on {video_file}: {e}")

# ============================================================
# Main
# ============================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--video_dir", type=str, required=True, help="Path to videos (used for BOTH calibration and inference)")
    args = parser.parse_args()

    # Pass the video dir to the quantizer so it can find real data
    ensure_fp4_model(args.video_dir)
    run_inference(args.video_dir)